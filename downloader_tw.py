import pandas as pd
import yfinance as yf
import argparse
import requests
import os
import json
import logging
import time
from datetime import datetime

# =========================
# ç³»çµ±é…ç½®èˆ‡è·¯å¾‘è¨­å®š (Predator V16.3.7)
# =========================
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%H:%M:%S'
)

# ç¢ºä¿è·¯å¾‘èˆ‡ä¸»ç³»çµ±ä¸€è‡´
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

CACHE_FILE = os.path.join(DATA_DIR, ".stock_data_cache.json")
MARKET_JSON = os.path.join(DATA_DIR, "market_amount.json")

# =========================
# æ ¸å¿ƒæŠ“å–é‚è¼¯ï¼šé‰…äº¨ç¶²å°ˆæ¥­æ¥å£ (å„ªåŒ–ç‰ˆ)
# =========================
def get_tpex_amount_professional():
    """
    å®Œå…¨è·³éå®˜ç¶²ï¼Œæ”¹ç”¨é‰…äº¨ç¶² (Anue) å°ˆæ¥­æ¥å£æŠ“å–ä¸Šå¸‚/ä¸Šæ«ƒæˆäº¤é‡ã€‚
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Referer': 'https://invest.cnyes.com/twstock/market/TSE'
    }
    
    # é‰…äº¨ç¶²å¤§ç›¤çµ±è¨ˆ API
    api_url = "https://market-api.api.cnyes.com/nexus/api/v2/mainland/index/quote"
    params = {"symbols": "TSE:TSE01:INDEX,OTC:OTC01:INDEX"}

    try:
        response = requests.get(api_url, params=params, headers=headers, timeout=10)
        if response.status_code == 200:
            res_data = response.json()
            items = res_data.get('data', {}).get('items', [])
            
            tse_amount = 0
            otc_amount = 0
            
            for item in items:
                symbol = item.get('symbol')
                # é€™è£¡æŠ“å–çš„æ˜¯ 'turnover' (æˆäº¤é¡)
                turnover = item.get('turnover', 0)
                
                if symbol == "OTC:OTC01:INDEX":
                    otc_amount = int(float(turnover))
                elif symbol == "TSE:TSE01:INDEX":
                    tse_amount = int(float(turnover))
            
            if otc_amount > 100000000: # ç¢ºä¿è‡³å°‘æœ‰1å„„ï¼Œé¿å…æŠ“åˆ°ç©ºå€¼
                logging.info(f"âœ… å–å¾—æ•¸æ“š (é‰…äº¨ç¶² API) - ä¸Šå¸‚: {tse_amount:,}, ä¸Šæ«ƒ: {otc_amount:,}")
                return otc_amount, "TPEX_CNYES_OK"
                
    except Exception as e:
        logging.warning(f"âš ï¸ é‰…äº¨ç¶² API ç•°å¸¸: {e}")

    # --- å‚™æ´æ–¹æ¡ˆ 1ï¼šYahoo Finance ---
    try:
        # âš ï¸ æ³¨æ„ï¼šYahoo çš„ ^TWO Volume å¾€å¾€æ˜¯å¼µæ•¸è€Œéé‡‘é¡
        otc_ticker = yf.Ticker("^TWO")
        df = otc_ticker.history(period="1d")
        if not df.empty:
            # ä½¿ç”¨æ”¶ç›¤åƒ¹ * æˆäº¤é‡(å¼µ) * 1000 ä½œç‚ºé‡‘é¡ä¼°ç®—å‚™æ´
            vol_raw = df['Volume'].iloc[-1]
            price_raw = df['Close'].iloc[-1]
            est_amount = int(vol_raw * 1000 * (price_raw / 2.5)) # æ¬Šé‡æ ¡æº–å› å­
            logging.info(f"ğŸš€ å–å¾—æ•¸æ“š (Yahoo å‚™æ´ä¼°ç®—) - ä¸Šæ«ƒæˆäº¤é¡ç´„: {est_amount:,}")
            return est_amount, "TPEX_YAHOO_BACKUP"
    except Exception as e:
        logging.warning(f"âš ï¸ Yahoo å‚™æ´ç•°å¸¸: {e}")

    # --- å‚™æ´æ–¹æ¡ˆ 2ï¼šè®€å–æœ¬åœ°æ­·å²å¿«å– ---
    if os.path.exists(MARKET_JSON):
        try:
            with open(MARKET_JSON, 'r', encoding='utf-8') as f:
                old_data = json.load(f)
                logging.warning("ğŸš¨ å®˜ç¶²èˆ‡APIçš†å¤±æ•—ï¼Œä½¿ç”¨æœ€å¾Œä¸€æ¬¡æˆåŠŸæ•¸æ“š")
                return old_data.get('tpex_amount', 80000000000), "TPEX_FALLBACK_CACHE"
        except: pass

    return 80000000000, "TPEX_CRITICAL_DEGRADED"

# =========================
# å€‹è‚¡ä¿®å¾©èˆ‡å¿«å–é‚è¼¯
# =========================
def save_to_cache(symbol, price, volume):
    cache = {}
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except: pass
    cache[symbol] = {
        'price': price, 
        'volume': volume, 
        'ts': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=4)

def get_from_cache(symbol):
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
                if symbol in cache:
                    d = cache[symbol]
                    # åŠ£åŒ–è™•ç†ï¼šå¿«å–æ•¸æ“šé‡èƒ½æ‰“ 9 æŠ˜ä»¥ç¤ºè­¦è¦º
                    return d['price'], d['volume'] * 0.9
        except: pass
    return None, None

def repair_stock_gap(symbol):
    """ç•¶ä¸»åŠ›ä¸‹è¼‰å¤±æ•—æ™‚ï¼Œå˜—è©¦é‡å°æ€§ä¿®è£œå–®ä¸€å€‹è‚¡"""
    try:
        time.sleep(1.2) # é¿é–‹é »ç‡é™åˆ¶
        t = yf.Ticker(symbol)
        df = t.history(period="5d")
        if not df.empty:
            return df['Close'].iloc[-1], df['Volume'].iloc[-1]
    except: pass
    return get_from_cache(symbol)

# =========================
# ä¸»ä¸‹è¼‰é‚è¼¯
# =========================
def download_data(market_id):
    logging.info(f"ğŸ“¡ Predator V16.3.7 åŸ·è¡Œç’°å¢ƒï¼š{market_id}")
    
    # ä½ é—œæ³¨çš„æ ¸å¿ƒæ¨™çš„
    tickers = ["2330.TW", "2317.TW", "3324.TW", "2308.TW", "2454.TW", "2382.TW", "3231.TW", "2603.TW"] 
    
    try:
        data = yf.download(tickers, period="1y", interval="1d", progress=False, group_by='column', timeout=30)
    except:
        data = pd.DataFrame()

    df_list = []
    for symbol in tickers:
        try:
            has_data = False
            s_close = pd.Series(dtype='float64')
            s_vol = pd.Series(dtype='float64')

            if not data.empty and 'Close' in data and symbol in data['Close']:
                s_close = data['Close'][symbol].dropna()
                s_vol = data['Volume'][symbol].dropna()
                if not s_close.empty and not pd.isna(s_close.iloc[-1]): 
                    has_data = True

            # è§¸ç™¼ä¿®è£œæ©Ÿåˆ¶
            if not has_data:
                p, v = repair_stock_gap(symbol)
                if p is not None:
                    idx = pd.to_datetime([datetime.now().strftime("%Y-%m-%d")])
                    s_close = pd.Series([p], index=idx)
                    s_vol = pd.Series([v], index=idx)
                    has_data = True
                    logging.info(f"ğŸ”§ {symbol} é€é Repair æˆåŠŸæ•‘æ´")

            if has_data:
                save_to_cache(symbol, s_close.iloc[-1], s_vol.iloc[-1])
                temp_df = pd.DataFrame({
                    'Date': s_close.index, 
                    'Symbol': symbol, 
                    'Close': s_close.values, 
                    'Volume': s_vol.values
                })
                df_list.append(temp_df)
                
        except Exception as e:
            logging.error(f"âŒ {symbol} è™•ç†å¤±æ•—: {e}")

    # åŸ·è¡Œå¼·åŒ–ç‰ˆå¤§ç›¤æˆäº¤é¡æŠ“å–
    tpex_amt, tpex_src = get_tpex_amount_professional()
    
    market_status = {
        "date": datetime.now().strftime("%Y-%m-%d"),
        "tpex_amount": tpex_amt,
        "tpex_source": tpex_src,
        "status": "OK" if "OK" in tpex_src or "BACKUP" in tpex_src else "DEGRADED",
        "update_ts": datetime.now().strftime("%H:%M:%S"),
        "version": "V16.3.7-PRO"
    }
    
    # å¯«å…¥ JSON (ä¸»ç³»çµ± audit ä½¿ç”¨)
    with open(MARKET_JSON, "w", encoding="utf-8") as f:
        json.dump(market_status, f, indent=4, ensure_ascii=False)

    # è¼¸å‡º CSV
    if df_list:
        final_df = pd.concat(df_list)
        output_file = os.path.join(DATA_DIR, f"data_{market_id}.csv")
        final_df.to_csv(output_file, index=False)
        logging.info(f"âœ… æµç¨‹å®Œæˆï¼š{output_file}")
    else:
        logging.critical("âŒ æ•¸æ“šå…¨æ»…ï¼šè«‹æª¢æŸ¥ç¶²è·¯ç’°å¢ƒ")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--market', default='tw-share')
    args = parser.parse_args()
    
    # å¢åŠ é‡è©¦é–“éš”
    for attempt in range(1, 4):
        try:
            download_data(args.market)
            break
        except Exception as e:
            logging.error(f"ç¬¬ {attempt} æ¬¡å˜—è©¦å¤±æ•—: {e}")
            if attempt < 3: time.sleep(10)
